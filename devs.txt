git clone https://github.com/nikakostava/btutask.gitcd btutaskdocker build -t python 
.trivy image python 
python:3.13.0a1-alpine3.18

sudo snap install trivy

VIIA hybrid cloud is one in which
applications are running in a
combination of different
environments. Hybrid cloud
computing approaches are
widespread because almost no
one today relies entirely on the
public cloud.

Effective application governance

A hybrid approach allows you to
decide where your application sits
and where hybrid computing
happens. This can help improve
privacy and ensure compliance for
your regulated applications.

Improved performance and
reduced latency

Sometimes, distributed apps at
remote locations benefit from a
hybrid cloud solution. For
applications with low latency
requirements, hybrid computing
happens close to the end users.

Hybrid cloud security overview

Hybrid cloud security is the protection of the data, applications, and
infrastructure associated with an IT architecture that incorporates some
degree of workload portability, orchestration, and management across
multiple IT environments, including at least 1
cloud—public or private.

Hybrid clouds offer the opportunity to reduce the potential exposure of
your data. You can keep sensitive or critical data off the public cloud
while still taking advantage of the cloud for data that doesn’t have the
same kinds of risk associated with it.

Hybrid cloud security challenges

Protecting your data

Limit data exposure for your organization through encryption. The same data will be
either in transit or at rest at different moments in time. You need a variety of
security to limit data exposure during either of these states.

Compliance and governance

If you work in a highly regulated sector like healthcare, finances, or government,
hybrid cloud infrastructure may present additional considerations. Know how to
check your distributed environments to make sure that they are compliant; how to
implement custom or regulatory security baselines; and how to prepare for security
audits.

Security in the supply chain

Hybrid cloud environments often include products and software from multiple
vendors in a complicated ecosystem. Know how your vendors test and manage
their software and products. Understand when and how your vendors have
inspected source code, how and which implementation guidelines they follow, and
how and when vendors can provide updates and patches

The components of hybrid cloud security

Hybrid cloud security, like computer security in general, consists of
three components: physical, technical, and administrative.

Physical controls are for securing your actual hardware. Examples
include locks, guards, and security cameras.

Technical controls are protections designed into IT systems
themselves, such as encryption, network authentication, and
management software. Many of the strongest security tools for hybrid
cloud are technical controls.

Finally, administrative controls are programs to help people act in
ways that enhance security, such as training and disaster planning.

Physical controls for hybrid cloud security

Hybrid clouds can span multiple locations, which makes physical
security a special challenge. You can’t build a perimeter around all
your machines and lock the door.

In the case of shared resources like a public cloud, you may have
Service Level Agreements (SLAs) with your cloud provider that
define which physical security standards will be met. For example,
some public cloud providers have arrangements with government
clients to restrict which personnel have access to the physical
hardware.

But even with good SLAs, you’re giving up some level of control
when you’re relying on a public cloud provider. This means other
security controls become even more important.

Technical controls for hybrid cloud
security

Technical controls are the heart of hybrid cloud security. The
centralized management of a hybrid cloud makes technical controls
easier to implement.

Some of the most powerful technical controls in your hybrid cloud
toolbox are encryption, automation, orchestration, access control, and
endpoint security.

Encryption

Encryption greatly reduces the risk that any readable data would be
exposed even if a physical machine is compromised.

You can encrypt data at rest and data in motion

Full disk (partition encryption) protects your data while your computer
is off. Try the Linux Unified Key Setup-on-disk (LUSK) format which
can encrypt your hard drive partitions in bulk.

Hardware encryption that will protect the hard drive from
unauthorized access. Try the Trusted Platform Module (TPM), which
is a hardware chip that stores cryptographic keys. When the TPM is
enabled, the hard drive is locked until the user is able to authenticate
their login.

Encrypt root volumes without manually entering your passwords. If
you have built a highly automated cloud environment, build upon that
work with automated encryption. If you are using Linux, try the
Network Bound Disk Encryption (NBDE), which works on both
physical and virtual machines. Bonus: make TPM part of the NBDE
and provide two layers of security (the NMDE will help protect
networked environments, while the TPM will work on premises).

Encrypt your network session. Data in motion is at a much higher risk
of interception and alteration. Try the Internet Protocol Security
(IPsec) which is an extension of the Internet Protocol that uses
cryptography.

Select products that already implement security standards. Look for
products that support the Federal Information Processing Standard
(FIPS) Publication 140-2 which uses cryptographic modules to protect
high-risk data.

Automation

To appreciate why automation is a natural fit for hybrid clouds,
consider the drawbacks of manual monitoring and patching.

Manual monitoring for security and compliance often has more risks
than rewards. Manual patches and configuration management risk
being implemented asynchronously. It also makes implementing
self-service systems more difficult. If there is a security breach,
records of manual patches and configurations risk being lost and can
lead to team in-fighting and finger-pointing. Additionally, manual
processes tend to be more error prone and take more time.

Automation, by contrast, allows you to stay ahead of risks, rather than
react to them. Automation gives you the ability to set rules, share, and
verify processes which ultimately make it easier to pass security audits

Monitoring your environments

Managing your data

Checking for compliance

Implementing patches

Implementing custom or regulatory security baselines

Orchestration

Cloud orchestration goes a step further. You can think of automation as
defining specific ingredients, and orchestration as a cookbook of recipes that
bring the ingredients together.

Orchestration makes it possible to manage cloud resources and their
software components as a single unit, and then deploy them in an
automated, repeatable way through a template.

Orchestration’s biggest boon to security is standardization. You can deliver
the flexibility of the cloud while still making sure the systems deployed
meet your standards for security and compliance.

Access control

Hybrid clouds also depend on access control. Restrict user accounts to only the
privileges they need and consider requiring two-factor authentication. Limiting
access to users connected to a Virtual Private Network (VPN) can also help
you maintain security standards.

Endpoint security

Endpoint security often means using software to remotely revoke access or
wipe sensitive data if a user’s smartphone, tablet, or computer gets lost, stolen,
or hacked.

Users can connect to a hybrid cloud with personal devices from anywhere,
making endpoint security an essential control. Adversaries may target your
systems with phishing attacks on individual users and malware that
compromises individual devices.

We’re listing it here as a technical control, but endpoint security combines
physical, technical and administrative controls: Keep physical devices secure,
use technical controls to limit the risks if a device falls into the wrong hands,
and train users in good security practices.

Administrative controls for hybrid
cloud security

Lastly, administrative controls in hybrid cloud security are implemented to
account for human factors. Because hybrid cloud environments are highly
connected, security is every user’s responsibility.

Disaster preparedness and recovery are an example of an administrative
control. If part of your hybrid cloud is knocked offline, who’s responsible
for what actions? Do you have protocols in place for data recovery?

Hybrid architecture offers significant advantages for administrative security.
With your resources potentially distributed among on-site and off-site
hardware, you have options for backups and redundancies. In hybrid clouds
that involve public and private clouds, you can fail over to the public cloud
if a system on your private data center cloud fails.


Vi
What is cloud computing

Cloud computing is a general term for anything that involves delivering
hosted services over the internet. These services are divided into three
main categories or types of cloud computing: infrastructure as a service
(IaaS), platform as a service (PaaS) and software as a service (SaaS).

A cloud can be private or public. A public cloud sells services to anyone on
the internet. A private cloud is a proprietary network or a data center that
supplies hosted services to a limited number of people, with certain access
and permissions settings. Private or public, the goal of cloud computing is
to provide easy, scalable access to computing resources and IT services.

Cloud infrastructure involves the hardware and software components
required for proper implementation of a cloud computing model. Cloud
computing can also be thought of as utility computing or on-demand
computing.

The name cloud computing was inspired by the cloud symbol that's often
used to represent the internet in flowcharts and diagrams.

How does cloud computing work

Cloud computing works by enabling client devices to access data and cloud 
applications
over the internet from remote physical servers, databases and computers.

An internet network connection links the front end, which includes the accessing 
client
device, browser, network and cloud software applications, with the back end, which 
consists
of databases, servers and computers. The back end functions as a repository, storing 
data
that is accessed by the front end.

Communications between the front and back ends are managed by a central server. The
central server relies on protocols to facilitate the exchange of data. The central 
server uses
both software and middleware to manage connectivity between different client devices 
and
cloud servers. Typically, there is a dedicated server for each individual application 
or
workload.

Cloud computing relies heavily on virtualization and automation technologies. 
Virtualization
enables the easy abstraction and provisioning of services and underlying cloud 
systems
into logical entities that users can request and utilize. Automation and
accompanying orchestration capabilities provide users with a high degree of 
self-service to
provision resources, connect services and deploy workloads without direct 
intervention from
the cloud provider's IT staff.

Types of cloud computing services

IaaS. IaaS providers, such as Amazon Web Services (AWS), supply a virtual
server instance and storage, as well as application programming interfaces (APIs) 
that let
users migrate workloads to a virtual machine (VM). Users have an allocated storage
capacity and can start, stop, access and configure the VM and storage as desired. 
IaaS
providers offer small, medium, large, extra-large, and memory- or compute-optimized
instances, in addition to enabling customization of instances, for various workload 
needs.
The IaaS cloud model is closest to a remote data center for business users.

PaaS. In the PaaS model, cloud providers host development tools on their 
infrastructures.
Users access these tools over the internet using APIs, web portals or gateway 
software.
PaaS is used for general software development, and many PaaS providers host the
software after it's developed. Common PaaS products include Salesforce's Lightning
Platform, AWS Elastic Beanstalk and Google App Engine.

SaaS. SaaS is a distribution model that delivers software applications over the 
internet;
these applications are often called web services. Users can access SaaS applications 
and
services from any location using a computer or mobile device that has internet 
access. In
the SaaS model, users gain access to application software and databases. One common
example of a SaaS application is Microsoft 365 for productivity and email services.

Cloud computing deployment models

Private cloud services are delivered from a business's data center to internal users. 
With a
private cloud, an organization builds and maintains its own underlying cloud 
infrastructure.
This model offers the versatility and convenience of the cloud, while preserving the
management, control and security common to local data centers. Internal users might 
or
might not be billed for services through IT chargeback. Common private cloud 
technologies
and vendors include VMware and OpenStack.

In the public cloud model, a third-party cloud service provider (CSP) delivers the 
cloud
service over the internet. Public cloud services are sold on demand, typically by the 
minute
or hour, though long-term commitments are available for many services. Customers only
pay for the central processing unit cycles, storage or bandwidth they consume. 
Leading
public CSPs include AWS, Microsoft Azure, IBM and Google Cloud Platform (GCP), as 
well
as IBM, Oracle and Tencent.

A hybrid cloud is a combination of public cloud services and an on-premises private 
cloud,
with orchestration and automation between the two. Companies can run mission-critical
workloads or sensitive applications on the private cloud and use the public cloud to 
handle
workload bursts or spikes in demand. The goal of a hybrid cloud is to create a 
unified,
automated, scalable environment that takes advantage of all that a public cloud
infrastructure can provide, while still maintaining control over mission-critical 
data.

Characteristics and advantages of
cloud computing

Self-service provisioning. End users can spin up compute resources for almost any 
type of workload on demand. An end user can
provision computing capabilities, such as server time and network storage, 
eliminating the traditional need for IT administrators to
provision and manage compute resources.

Elasticity. Companies can freely scale up as computing needs increase and scale down 
again as demands decrease. This eliminates
the need for massive investments in local infrastructure, which might or might not 
remain active.

Pay per use. Compute resources are measured at a granular level, enabling users to 
pay only for the resources and workloads they
use.

Workload resilience. CSPs often implement redundant resources to ensure resilient 
storage and to keep users' important workloads
running -- often across multiple global regions.

Migration flexibility. Organizations can move certain workloads to or from the cloud 
-- or to different cloud platforms -- as desired or
automatically for better cost savings or to use new services as they emerge.

Broad network access. A user can access cloud data or upload data to the cloud from 
anywhere with an internet connection using
any device.

Multi-tenancy and resource pooling. Multi-tenancy lets numerous customers share the 
same physical infrastructures or the same
applications yet still retain privacy and security over their own data. With resource 
pooling, cloud providers service numerous customers
from the same physical resources. The resource pools of the cloud providers should be 
large and flexible enough so they can service
the requirements of multiple customers.

Cost management. Using cloud infrastructure can reduce capital costs, as 
organizations
don't have to spend massive amounts of money buying and maintaining equipment. This
reduces their capital expenditure costs -- as they don't have to invest in hardware, 
facilities,
utilities or building large data centers to accommodate their growing businesses.
Additionally, companies don't need large IT teams to handle cloud data center 
operations
because they can rely on the expertise of their cloud providers' teams. Cloud 
computing
also cuts costs related to downtime. Since downtime rarely happens in cloud 
computing,
companies don't have to spend time and money to fix any issues that might be related 
to
downtime.

Data and workload mobility. Storing information in the cloud means that users can 
access
it from anywhere with any device with just an internet connection. That means users 
don't
have to carry around USB drives, an external hard drive or multiple CDs to access 
their
data. Users can access corporate data via smartphones and other mobile devices, 
enabling
remote employees to stay up to date with co-workers and customers. End users can 
easily
process, store, retrieve and recover resources in the cloud. In addition, cloud 
vendors
provide all the upgrades and updates automatically, saving time and effort.

Business continuity and disaster recovery (BCDR). All organizations worry about data
loss. Storing data in the cloud guarantees that users can always access their data 
even if
their devices, e.g., laptops or smartphones, are inoperable. With cloud-based 
services,
organizations can quickly recover their data in the event of emergencies, such as 
natural
disasters or power outages. This benefits BCDR and helps ensure that workloads and 
data
are available even if the business suffers damage or disruption.

Disadvantages of cloud computing

Cloud security. Security is often considered the greatest challenge facing cloud 
computing. When relying on the cloud, organizations risk data breaches, hacking
of APIs and interfaces, compromised credentials and authentication issues. 
Furthermore, there is a lack of transparency regarding how and where sensitive
information entrusted to the cloud provider is handled. Security demands careful 
attention to cloud configurations and business policy and practice.

Cost unpredictability. Pay-as-you-go subscription plans for cloud use, along with 
scaling resources to accommodate fluctuating workload demands, can make it
tough to define and predict final costs. Cloud costs are also frequently 
interdependent, with one cloud service often utilizing one or more other cloud 
services -- all
of which appear in the recurring monthly bill. This can create additional unplanned 
cloud costs.

Lack of capability and expertise. With cloud-supporting technologies rapidly 
advancing, organizations are struggling to keep up with the growing demand for
tools and employees with the proper skill sets and knowledge needed to architect, 
deploy, and manage workloads and data in a cloud.

IT governance. The emphasis on do-it-yourself capability in cloud computing can make 
IT governance difficult, as there is no control over provisioning,
deprovisioning and management of infrastructure operations. This can make it 
challenging to properly manage risks and security, IT compliance and data quality.

Compliance with industry laws. When transferring data from on-premises local storage 
into cloud storage, it can be difficult to manage compliance with industry
regulations through a third party. It's important to know where data and workloads 
are actually hosted in order to maintain regulatory compliance and proper
business governance.

Management of multiple clouds. Every cloud is different, so multi-cloud deployments 
can disjoint efforts to address more general cloud computing challenges.

Cloud performance. Performance -- such as latency -- is largely beyond the control of 
the organization contracting cloud services with a provider. Network and
provider outages can interfere with productivity and disrupt business processes if 
organizations are not prepared with contingency plans.

Building a private cloud. Architecting, building and managing private clouds -- 
whether for its own purpose or for a hybrid cloud goal -- can be a daunting task for
IT departments and staff.

Cloud migration. The process of moving applications and other data to a cloud 
infrastructure often causes complications. Migration projects frequently take
longer than anticipated and go over budget. The issue of workload and data 
repatriation -- moving from the cloud back to a local data center -- is often 
overlooked
until unforeseen cost or performance problems arise.

Vendor lock-in. Often, switching between cloud providers can cause significant 
issues. This includes technical incompatibilities, legal and regulatory limitations
and substantial costs incurred from sizable data migrations.

cloud security

Cloud security, also known as cloud computing security, is the
practice of protecting cloud-based data, applications and
infrastructure from cyber attacks and cyber threats.

Cybersecurity, of which cloud security is a subset, has the same
goals. Where cloud security differs from traditional cybersecurity
is in the fact that administrators must secure assets that reside
within a third-party service provider's infrastructure.

Why cloud security is important

As enterprise cloud adoption grows, business-critical applications and data migrate 
to
trusted third-party cloud service providers (CSPs). Most major CSPs offer standard
cybersecurity tools with monitoring and alerting functions as part of their service 
offerings,
but in-house information technology (IT) security staff may find these tools do not 
provide
enough coverage, meaning there are cybersecurity gaps between what is offered in the
CSP's tools and what the enterprise requires. This increases the risk of data theft 
and loss.

Because no organization or CSP can eliminate all security threats and 
vulnerabilities,
business leaders must balance the benefits of adopting cloud services with the level 
of data
security risk their organizations are willing to take.

Putting the right cloud security mechanisms and policies in place is critical to 
prevent
breaches and data loss, avoid noncompliance and fines, and maintain business 
continuity
(BC).

A major benefit of the cloud is that it centralizes applications and data and 
centralizes the
security of those applications and data as well. Eliminating the need for dedicated 
hardware
also reduces organizations' cost and management needs, while increasing reliability,
scalability and flexibility.

How cloud security works

Public cloud services are hosted by CSPs. These include
software as a service (SaaS), platform as a service (PaaS) and
infrastructure as a service (IaaS).

Private clouds are hosted by or for a single organization.

Hybrid clouds include a mix of public and private clouds. As a
result, cloud security mechanisms take two forms: those
supplied by CSPs and those implemented by customers. It is
important to note that handling of security is rarely the complete
responsibility of the CSP or the customer. It is usually a joint
effort using a shared responsibility model.

The shared responsibility model

Although not standardized, the shared responsibility model is a
framework that outlines which security tasks are the obligation
of the CSP and which are the duty of the customer. Enterprises
using cloud services must be clear which security
responsibilities they hand off to their provider(s) and which they
need to handle in-house to ensure they have no gaps in
coverage.

Customers should always check with their CSPs to understand
what the provider covers and what they need to do themselves
to protect the organization.

CSP security responsibilities

Security controls supplied by CSPs vary by service model, be it SaaS, PaaS or
IaaS. Customer responsibility commonly increases from SaaS to PaaS to IaaS.

In general, CSPs are always responsible for servers and storage. They secure and
patch the infrastructure itself, as well as configure the physical data centers,
networks and other hardware that power the infrastructure, including virtual
machines (VMs) and disks. These are usually the sole responsibilities of CSPs in
IaaS environments.

In a PaaS environment, CSPs assume more responsibility, including securing
runtime, networking, operating systems (OSes), data and virtualization. In a SaaS
environment, CSPs also provide application and middleware security.

The details of security responsibilities can vary by provider and customer. For
example, CSPs with SaaS-based offerings may or may not offer customers
visibility into the security tools they use. IaaS providers, on the other hand, 
usually
offer built-in security mechanisms that enable customers to access and view CSP
security tools, which may also provide customer-alerting functionality.

Customer security responsibilities

To supplement the CSP security controls listed above, customers are generally 
responsible for application, middleware, virtualization,
data, OS, network and runtime security in IaaS clouds. In IaaS architectures, such as 
Amazon Virtual Private Cloud (VPC) or Microsoft
Azure Virtual Network (VNet), for example, customers can supplement, replace or 
overlay built-in cybersecurity mechanisms with their
own set of tools.

In PaaS environments, customers take on fewer security tasks, generally only 
application and middleware security. SaaS environments
involve even less customer responsibility.

Data security and identity and access management (IAM) are always the responsibility 
of the customer, however, regardless of cloud
delivery model. Encryption and compliance are also the responsibility of the 
customer.

Yet, because CSPs control and manage the infrastructure customer apps and data 
operate within, adopting additional controls to
further mitigate risk can be challenging. IT security staff should get involved as 
early as possible when evaluating CSPs and cloud
services. Security teams must evaluate the CSP's default security tools to determine 
whether additional measures will need to be
applied in-house.

Adding a company's own security tools to cloud environments is typically done by 
installing one or more network-based virtual
security appliances. Customer-added tool sets enable security administrators to get 
granular with specific security configurations and
policy settings. Many enterprises also often find it cost-effective to implement the 
same tools in their public clouds as they have within
their corporate local area networks (LANs). This prevents administrators from having 
to recreate security policies in the cloud using
disparate security tools. Instead, a single security policy can be created once and 
then pushed out to identical security tools, regardless
of whether they are on premises or in the cloud.

Cloud security tools

Many of the same tools used in on-premises environments
should be used in the cloud, although cloud-specific versions of
them may exist. These tools and mechanisms include
encryption, IAM and single sign-on (SSO), data loss prevention
(DLP), intrusion prevention and detection systems
(IPSes/IDSes) and public key infrastructure (PKI).

Some cloud-specific tools

Cloud workload protections platforms (CWPPs). A CWPP is a security mechanism designed 
to protect workloads -- for example, VMs, applications or data --
in a consistent manner.

Cloud access security brokers (CASBs). A CASB is a tool or service that sits between 
cloud customers and cloud services to enforce security policies and, as a
gatekeeper, add a layer of security.

Cloud security posture management (CSPM). CSPM is a group of security products and 
services that monitor cloud security and compliance issues and aim to
combat cloud misconfigurations, among other features.

Secure Access Service Edge (SASE) and zero-trust network access (ZTNA) are also 
emerging as two popular cloud security models/frameworks.

Security as a service, often shortened to SaaS or SECaaS, is a subset of software as 
a service. The Cloud Security Alliance (CSA) defined 10 SECaaS
categories:

IAM

DLP

web security

email security

security assessments

intrusion management

security information and event management (SIEM)

encryption

BC/disaster recovery (BCDR)

network security

How to secure data in the cloud

The steps required to secure data in the cloud vary. Factors, including the type and 
sensitivity of the data to be
protected, cloud architecture, accessibility of built-in and third-party tools, and 
number and types of users
authorized to access the data must be considered.

Encrypt data at rest, in use and in motion.

Use two-factor authentication (2FA) or multifactor authentication (MFA) to verify 
user identity before granting
access.

Adopt cloud edge security protections, including firewalls, IPSes and antimalware.

Isolate cloud data backups to prevent ransomware threats.

Ensure data location visibility and control to identify where data resides and to 
implement restrictions on whether
data can be copied to other locations inside or outside the cloud.

Log and monitor all aspects of data access, additions and changes.

Emerging cybersecurity tools should also be considered to help secure data in clouds. 
These include network
detection and response (NDR) and artificial intelligence (AI) for IT operations 
(AIOps). Both tools collect cloud
infrastructure health and cybersecurity information. AI then analyzes data and alerts 
administrators of abnormal
behavior that could indicate a threat.

Top cloud security challenges

insider threats

data loss

data breaches

IAM

key management

access control

phishing

malware

shadow IT

distributed denial-of-service (DDoS) attacks

insecure application programming interfaces (APIs)

cloud account hijacking;

lack of cloud visibility and control;

working with cloud security tools that in-house administrators may be
unfamiliar with;

tracking and monitoring where data is located both in transit and at
rest;

misconfigurations;

weak cloud control plane;

challenges understanding the shared responsibility model;

nefarious use of cloud services;

multi-tenancy concerns;

incompatibilities with on-premises environments;

cloud compliance; and

cloud governance.

Cloud security best practices

Understand the shared responsibility model, including the responsibilities of your 
CSPs and
your security team.

Choose your CSPs wisely. Know what security controls they offer, and review contracts 
and
service-level agreements (SLAs) diligently.

Adopt a strong, granular IAM policy to control who has access to what. Employ the 
principle
of least privilege (POLP), and require strong passwords and 2FA or MFA.

Encrypt data in at rest, in use and in motion.

Maintain cloud visibility through continuous monitoring.

Understand cloud compliance requirements and regulations.

Establish and enforce cloud security policies.

Conduct security awareness training for employees, third-party partners and anyone
accessing organizational cloud resources.

Segment clouds and workloads.

What Is CSPM

Cloud Security Posture Management (CSPM) is a security solution category
that can identify and help remediate cloud misconfigurations. Gartner
introduced the CSPM category, and officially describes it as a solution that
automates security and provides compliance assurance in the cloud.

CSPM starts from a framework based on regulatory requirements, industry
benchmarks and company policies, and continuously manages cloud risk by
preventing, detecting, responding and predicting cloud infrastructure risks.

At the heart of CSPM technology is active and passive detection and
assessment of risk in cloud service configurations, for example network and
storage configuration, and security settings, for example encryption and
assigned permissions. In many cases, if the configuration is not compliant,
the CSPM product can take automated corrective action to reconcile it with
compliance and security requirements.

What Are the Factors Driving the Need for
CSPM

The increasing adoption of cloud computing—enterprises are using more and more cloud 
services, utilizing an increasingly
complex environment. To satisfy more needs, cloud vendors offer a multitude of 
services, each with its own configuration
options, making it difficult to monitor and understand the security implications of 
those configurations.

Cloud visibility issues due to cloud sprawl—enterprises usually do not have complete 
visibility into their cloud deployments
and services. This is due to many teams having the capability to set up new cloud 
accounts and services, often across multiple
providers.

The dynamic nature of cloud infrastructure—cloud services offer scale-out 
capabilities that constantly add or remove new
resources.

Complex environments lead to security issues—as enterprises adopt multi-cloud and 
hybrid cloud strategies, visibility
becomes crucial for proper security.

Self-service IaaS and PaaS—self-service capabilities enable developers to eliminate 
their reliance on IT and security personnel
during planning and deployment phases but do not provide adequate security and 
visibility coverage.

Lack of security expertise—developers and cloud operations teams are not security 
experts but are required to make risk and
security decisions regarding aspects like encryption, service authorization, and key 
management. They must have adequate
visibility and control to avoid mistakes and misconfigurations.

No adequate tooling—while enterprises shift to DevOps to increase speed, traditional 
security tools are too slow and
cumbersome to manage cloud risk. Additionally, teams must integrate compliance and 
security checks directly into development
pipelines.

How Does Cloud Security Posture
Management Work

CSPM tools work by examining the cloud environment and comparing it to best practices 
and known security
issues. CSPM tools alert the owner of a cloud resource when security risks need to be 
fixed, and in some cases
use automation to remediate issues automatically, such as revoking inappropriate 
account privileges.

CSPM is typically used by organizations adopting a cloud-first strategy and wanting 
to extend security best
practices to hybrid and multi-cloud environments. CSPM was originally used to secure
Infrastructure-as-a-Service (IaaS) cloud resources, such as Amazon EC2 compute 
instances, but can be used to
identify misconfigurations in platform as a service (PaaS), such as cloud databases, 
as well as
software-as-a-service (SaaS).

CSPM solutions may rely on several data sources. Typically, the cloud provider APIs 
are accessed to gain
visibility into service configurations. Other sources might include cloud monitoring 
of events (e.g., AWS
CloudTrail), log analysis, or analysis of cloud block storage volumes in order to 
find vulnerable workloads.

Some CSPM tools can only use best practices defined for a specific cloud environment 
or service, while others
are more flexible, letting the organization specify custom compliance standards or 
policies. This is important to
consider when selecting tools—because specific tools may be limited to detecting 
misconfigurations in specific
cloud environments, and may not work across multiple cloud accounts.

Most CSPM tools support continuous compliance checks according to common regulations 
and industry
standards, including HIPAA, GDPR, and PCI DSS.

CSPM Security Benefits and Risks

High visibility of security policies and consistent enforcement across multiple 
clouds

Real time discovery and security checks for new cloud workloads and services

Alerting about new, risky deployments or changes to a cloud environment

Cloud risk management, risk visualization and risk prioritization

Oversight over operational activities

However, CSPM tools also have limitations:

Cannot evaluate and secure shadow IaaS/PaaS/SaaS deployments.

Might not understand the context of the data, for example, if data is sensitive or if 
an
application could be malicious.

Generate a large number of alerts, which requires skilled cloud security experts to 
interpret
and respond to. These experts are in short supply. Newer approaches mitigate this 
alert
fatigue



V
What is a Cyber Attack

When there is an unauthorized system/network access by a
third party, we term it as a cyber attack. The person who carries
out a cyberattack is termed as a hacker/attacker.

Cyber-attacks have several negative effects. When an attack is
carried out, it can lead to data breaches, resulting in data loss or
data manipulation. Organizations incur financial losses,
customer trust gets hampered, and there is reputational
damage. To put a curb on cyberattacks, we
implement cybersecurity. Cybersecurity is the method of
safeguarding networks, computer systems, and their
components from unauthorized digital access.

Malware Attack

“Malware” refers to malicious software viruses including
worms, spyware, ransomware, adware, and trojans.

The trojan virus disguises itself as legitimate software.
Ransomware blocks access to the network's key components,
whereas Spyware is software that steals all your confidential
data without your knowledge. Adware is software that displays
advertising content such as banners on a user's screen.

Malware breaches a network through a vulnerability. When the
user clicks a dangerous link, it downloads an email attachment
or when an infected pen drive is used.

How we can prevent a malware attack

Use antivirus software. It can protect your computer against
malware.

Use firewalls. Firewalls filter the traffic that may enter your
device.

Stay alert and avoid clicking on suspicious links.

Update your OS and browsers, regularly.

Phishing Attack

Phishing attacks are one of the most prominent widespread
types of cyberattacks. It is a type of social engineering attack
wherein an attacker impersonates to be a trusted contact and
sends the victim fake mails.

Unaware of this, the victim opens the mail and clicks on the
malicious link or opens the mail's attachment. By doing so,
attackers gain access to confidential information and account
credentials. They can also install malware through a phishing
attack.

How we can prevent Phishing Attack

Most phishing emails have significant errors like spelling
mistakes and format changes from that of legitimate sources.

Make use of an anti-phishing toolbar.

Update your passwords regularly.

Security tranings

Password Attack

It is a form of attack wherein a hacker cracks your password with various
programs and password cracking tools like Aircrack, Cain, Abel, John the Ripper,
Hashcat, etc. There are different types of password attacks like brute force
attacks, dictionary attacks, and keylogger attacks.

Listed below are a few ways to prevent password attacks:

Use strong alphanumeric passwords with special characters.

Abstain from using the same password for multiple websites or accounts.

Update your passwords; this will limit your exposure to a password attack.

Do not have any password hints in the open.

Man-in-the-Middle Attack

A Man-in-the-Middle Attack (MITM) is also known as an
eavesdropping attack. In this attack, an attacker comes in between a
two-party communication, i.e., the attacker hijacks the session
between a client and host. By doing so, hackers steal and manipulate
data.

As seen below, the client-server communication has been cut off, and
instead, the communication line goes through the hacker.

MITM attacks can be prevented by following the below-mentioned
steps:

Be mindful of the security of the website you are using. Use encryption on
your devices.

Refrain from using public Wi-Fi networks.

SQL Injection Attack

A Structured Query Language (SQL) injection attack occurs on a
database-driven website when the hacker manipulates a standard SQL
query. It is carried by injecting a malicious code into a vulnerable website
search box, thereby making the server reveal crucial information.

This results in the attacker being able to view, edit, and delete tables in the
databases. Attackers can also get administrative rights through this.

To prevent a SQL injection attack:

Use an Intrusion detection system, as they design it to detect unauthorized access
to a network.

Carry out a validation of the user-supplied data. With a validation process, it keeps
the user input in check

Denial-of-Service Attack

A Denial-of-Service Attack is a significant threat to companies. Here, attackers
target systems, servers, or networks and flood them with traffic to exhaust their
resources and bandwidth.

When this happens, catering to the incoming requests becomes overwhelming for
the servers, resulting in the website it hosts either shut down or slow down. This
leaves the legitimate service requests unattended.

It is also known as a DDoS (Distributed Denial-of-Service) attack when attackers
use multiple compromised systems to launch this attack.

Let’s now look at how to prevent a DDoS attack:

Run a traffic analysis to identify malicious traffic.

Understand the warning signs like network slowdown, intermittent website shutdowns, 
etc.
At such times, the organization must take the necessary steps without delay.

Formulate an incident response plan, have a checklist and make sure your team and 
data
center can handle a DDoS attack.

Outsource DDoS prevention to cloud-based service providers.

supply chain attack

A supply chain attack, also called a value-chain or third-party attack,
occurs when someone infiltrates your system through an outside
partner or provider with access to your systems and data. This has
dramatically changed the attack surface of the typical enterprise in the
past few years, with more suppliers and service providers touching
sensitive data than ever before.

The risks associated with a supply chain attack have never been
higher, due to new types of attacks, growing public awareness of the
threats, and increased oversight from regulators. Meanwhile, attackers
have more resources and tools at their disposal than ever before,
creating a perfect storm.

Zero Day Attack

A zero-day attack happens once that flaw, or software/hardware
vulnerability, is exploited and attackers release malware before a
developer has an opportunity to create a patch to fix the
vulnerability—hence “zero-day.”

A company’s developers create software, but unbeknownst to them it
contains a vulnerability.

The threat actor spots that vulnerability either before the developer does or
acts on it before the developer has a chance to fix it.

The attacker writes and implements exploit code while the vulnerability is still
open and available

After releasing the exploit, either the public recognizes it in the form of
identity or information theft, or the developer catches it and creates a patch to
staunch the cyber-bleeding.

Microservices

Lack of Resources & Rate Limiting

Quite often, APIs do not impose any restrictions on the size or
number of resources that can be requested by the client/user.
Not only can this impact the API server performance, leading
to Denial of Service (DoS), but also leaves the door open to
authentication flaws such as brute force.

Example Scenarios

User bombards the API with millions of requests in a short span of time, forcing 
services to reach its
resource limits and leads to DDOS.

Product Catalog GET API returns the list of products but it relies on client API 
(implementing
pagination) to send the page limit. An attacker can set the limit to a large number 
which can push the
service to return a huge payload. Service consumes all of its networks and compute 
resources for one
request and becomes unavailable for other requests.

User Profile API accepts the profile pic but does not limit the size of it. An 
attacker can send a large file
and again bring down the service performance.

Solutions

Limit the “request payload size” for the service. If the parameter is a string, set 
the maximum length.
If the parameter is an array set the maximum number of elements. If the parameter is 
of type media
or document set the maximum size.

Limit the “number of request rate per client ” based on a specific user, IP address, 
or the calling
service.

Limit the “number of records per response ” at the service side. This limit is 
independent of what the
limit is coming from the client/user end.

Limit the Memory — The limit on memory cannot be set at the code level. If you are 
using containers
to deploy your services, this is piece of cake though. Docker provides easy options 
to do it.

Limit the CPU cycles — Similar to the memory, this limit can also be set through 
Docker. You can
define the “maximum CPU” cycles (of a host) a particular container can consume

Security Misconfigurations

Security misconfiguration is commonly a result of unsecure
default configurations, incomplete or ad-hoc configurations,
open cloud storage, misconfigured HTTP headers, unnecessary
HTTP methods

Example Scenarios

TLS is configured for Server/Services/API Gateway but they can still be accessed 
through plain HTTP.

Database Server is running with its default configuration where authentication is 
disabled by default.

Docker is running with default user — root.

Solutions

A repeatable hardening process should be applied to each technology and platform in 
the
ecosystem including microservices, cloud services, databases, files/storage, open 
id/OAuth servers,
application servers, container/container orchestration services, service mesh 
frameworks, etc.

Default configurations should be well inspected and updated as needed.

All network communications should be secured including client-APIs, API interactions, 
and media
access, based on TLS, Cors policies, firewall configuration, etc.

A task to review and update configurations across the entire API stack should be in 
place enabling
the continuous deployment of services. Ensure continuous & automated assessment of 
the
configurations.

To prevent exception traces and other valuable information from being sent back to 
attackers, if
applicable, define and enforce all API response payload schemas including error 
responses.

Ensure API can only be accessed by the specified HTTP verbs. All other HTTP verbs 
should be
disabled (e.g. HEAD).

APIs expecting to be accessed from browser-based clients (e.g., WebApp front-end) 
should
implement a proper Cross-Origin Resource Sharing (CORS) policy.

Best practices for microservices
security

Defense in Depth Mechanism

As microservices are known to adopt any mechanism on a granular
level, you can apply the Defense in Depth mechanism to make the
services more secure. In layman terms, the Defense in Depth
mechanism is basically a technique through which you can apply
layers of security countermeasures to protect the sensitive services.
So, you just have to identify the services with the most sensitive
information and then apply a number of security layers to protect
them. In this way, you can make sure that any potential attacker
cannot crack the security on a single go, and has to go forward and try
to crack the defense mechanism of all the layers.

Best practices for microservices
security

Tokens and API Gateway

Often, when you open an application, you see a dialog box saying, “Accept the License
Agreement and permission for cookies”. What does this message signify? Well, once you
accept it, your user credentials will be stored and a session will be created. Now, 
the next
time you go on the same page, the page will be loaded from the cache memory rather 
than
the servers itself. Before this concept came into the picture, sessions were stored 
on the
server-side centrally. But, this was one of the biggest barriers in horizontally 
scaling, the
application.

Tokens

So, the solution to this problem is to use tokens, to record the user credentials. 
These
tokens are used to easily identify the user and are stored in the form of cookies. 
Now, each
time a client requests a web page, the request is forwarded to the server, and then, 
the
server determines whether the user has access to the requested resource or not.

Now, the main problem is tokens where the user information is stored. So, the data of
tokens need to be encrypted to avoid any exploitation from 3 rd party resources. 
Jason Web
Format or most commonly known as JWT is an open standard that defines the token
format, provides libraries for various languages, and also encrypts those tokens.

Best practices for microservices
security

API Gateways

API Gateways add as an extra element to secure services through token
authentication. The API Gateway acts an entry point to all the client
requests and efficiently hides the microservices from the client. So,
the client has no direct access to microservices and thus in that way,
no client can exploit any of the services.


IV
SAST vs IAST

Interactive Application Security Testing (IAST) combines some of the
top features of DAST and SAST. Its purpose is to provide application
security testing within the application, typically during the
development phase. When configured correctly, IAST solutions offer
the following capabilities:

Accessing all the code in an application.

Collecting runtime application information about data flow and control.

Accessing configuration data.

Monitoring network traffic.

Accessing application components such as data, libraries, and frameworks in
dependencies at the back end.

SAST vs IAST

Compared to SAST and DAST, IAST processes more code, provides more
reliable results, and generates a more comprehensive view of applications
and their environments to identify security vulnerabilities.

IAST solutions can perform code scans like SAST products, allowing them
to discover vulnerabilities quickly and support early code fixes. Developers
can address coding issues sooner to avoid higher costs and delays.

However, the main problem with IAST is that it uses software agents. The
instrumentation code snippets are usually small and generally harmless, but
they can affect the behavior of applications and slow down performance.
Applying IAST tools can cause problems for highly performance-sensitive
apps—a long-term concern for agents.

DAST

Dynamic Application Security Testing (DAST) is a procedure that
actively investigates running applications with penetration tests to
detect possible security vulnerabilities.

Web applications power many mission-critical business processes
today, from public-facing e-commerce stores to internal financial
systems. While these web applications can enable dynamic business
growth, they also often harbor potential weaknesses that, if left
unidentified and unremediated, could quickly lead to a damaging and
costly data breach.

How does DAST work

DAST works by simulating automated attacks on an application,
mimicking a malicious attacker. The goal is to find outcomes or results
that were not expected and could therefore be used by attackers to
compromise an application. Since DAST tools don’t have internal
information about the application or the source code, they attack just
as an external hacker would—with the same limited knowledge and
information about the application.

What problems does DAST solve

Applications run the world economy and organizations are under
tremendous pressure to stay ahead of the curve as our digital world
accelerates. Businesses must continually innovate in an environment where
sophisticated, relentless threat actors are ready to exploit any opportunity to
disrupt, threaten critical data, and do damage. To successfully navigate this
new world, it is vital to develop and execute a plan to ensure their
applications are secure.

DAST works by simulating automated attacks on an application, mimicking
a malicious attacker. The goal is to find outcomes or results that were not
expected and could therefore be used by attackers to compromise an
application. Since DAST tools don’t have internal information about the
application or the source code, they attack just as an external threat actor
would—with the same limited knowledge and information about the
application.

Why is DAST vital to application security

As more businesses rely on web and mobile applications for success, application 
security vulnerabilities have
rapidly become the most prevalent cause of data breaches. Thus, it is more important 
than ever for organizations
to protect their applications and code.

Challenges that organizations are currently facing

The shift to the cloud and cloud-native application technologies is making 
applications more complex.

Massively distributed microservices and serverless functions mean that developers are 
focused solely on their
own services, and no one has a complete grasp of the entire codebase.

As the sheer number of applications increases, the overall lines of software code 
deployed to the cloud expands
the potential attack surface.

With more organizations focused on digital transformation, knowledge of the legacy 
code is waning as
developers retire or change roles.

The prevalence of third-party and open source software make applications more 
composite in nature. As a result,
a significant amount of the application code is developed outside the purview of the 
organization.

DevOps methodologies help development teams move faster but leave little time for 
manual or outdated security
checks.

How are DAST and SAST different

What Is Runtime Application Self-Protection

Runtime Application Self Protection (RASP) is a security solution
designed to provide personalized protection to applications. It takes
advantage of insight into an application’s internal data and state to
enable it to identify threats at runtime that may have otherwise been
overlooked by other security solutions.

How RASP Works

RASP wraps around and protects a particular application, rather than a
general network-level or endpoint-level defensive solution. This more
targeted deployment location enables RASP to monitor the inputs, outputs,
and internal state of the application that it is protecting. By deploying
RASP, developers can identify vulnerabilities within their applications.
Additionally, the RASP solution can block attempts to exploit existing
vulnerabilities in deployed applications.

RASP’s focused monitoring makes it capable of detecting a wide range of
threats, including zero-day attacks. Since RASP has insight into the internals
of an application, it can detect behavioral changes that may have been
caused by a novel attack. This enables it to respond to even zero-day attacks
based upon how they affect the target application.

Benefits of Runtime Application
Self-Protection

RASP differs from other cybersecurity solutions in its level of focus on a single 
application. This focus
enables it to provide a number of security benefits:

Contextual Awareness: When a RASP solution identifies a potential threat, it has 
additional contextual
information about the current state of the application and what data and code is 
affected. This context can be
invaluable for investigating, triaging, and remediating potential vulnerabilities 
since it indicates where the
vulnerability is located in the code and exactly how it can be exploited.

Visibility into Application-Layer Attacks: RASP has deep visibility into the 
application layer
because it is integrated with a particular application. This application-layer 
visibility, insight, and knowledge
can help to detect a wider range of potential attacks and vulnerabilities.

Zero-Day Protection: While RASP can use signatures to identify attacks, it is not 
limited to
signature-based detection. By identifying and responding to anomalous behaviors 
within the protected
application, RASP can detect and block even zero-day attacks.

Lower False Positives: RASP has deep insight into an application’s internals, 
including the ability to see
how a potential attack affects the application’s execution. This dramatically 
increases RASP’s ability to
differentiate true attacks (which have a true negative impact on application 
performance and security) from
false positives (such as SQL injection attempts that are never included in an SQL 
query). This reduction in
false positives decreases load on security teams and enables them to focus on true 
threats.

Benefits of Runtime Application
Self-Protection

Lower CapEx and OpEx: RASP is designed to be easy to deploy yet is able to make a 
significant difference in an
application’s vulnerability to attack and rate of false positive alerts. This 
combination reduces both up-front expenses
(CapEx) and the cost of effectively protecting the application (OpEx) compared to 
manual patching and web application
firewalls (WAFs).

Easy Maintenance: RASP works based upon insight into an application, not traffic 
rules, learning, or blacklists. SOC
teams love this reliability and CISOs appreciate the resource savings. Applications 
become self-protected and remain
protected wherever they go.

Cloud Support: RASP is designed to integrate with and be deployed as part of the 
application that it protects. This
enables it to be deployed in any location where the protected applications can run, 
including in the cloud.

Runtime Application Self-Protection (RASP)
Use Cases

Web Application Protection: Web applications and APIs are a crucial
component of an organization’s infrastructure but can be vulnerable to a wide
range of attacks. These applications are exposed to the public Internet and are often
prone to exploitable vulnerabilities. By deploying RASP to protect these
applications and APIs, an organization can limit the cybersecurity risk and attack
surface of its web-facing infrastructure.

Zero-Day Prevention: While an organization may have processes in place to
immediately apply patches for critical applications and systems, a patch can only
be applied after it is developed and released. RASP can be deployed to protect
critical applications within an organization (which may include web applications
and APIs) against zero-day vulnerabilities.

Cloud Application Protection: Securing the cloud can be complex because
applications run on leased infrastructure outside of the organization’s network
perimeter. Integrating RASP into these applications provides them with a high
level of security in a portable and largely infrastructure-agnostic form.

III
What is Container Image

A container image is a static file with executable code that can create a
container on a computing system. A container image is
immutable—meaning it cannot be changed, and can be deployed
consistently in any environment.

Container images include everything a container needs to run—the
container engine such as Docker or CoreOS, system libraries, utilities,
configuration settings, and specific workloads that should run on the
container. The image shares the operating system kernel of the host, so
it does not need to include a full operating system.

Docker Image Architecture

A Docker image is a collection of files, including binaries, source code
and other dependencies, needed to deploy a container environment. In
Docker, there are two ways to create an images:

Dockerfile—Docker provides a simple, human-readable configuration file that
specifies what a Docker image should contain.

Create an image from an existing container—you can run a container from
an existing image, modify the container environment, and save the result as a
new image.

Difference Between Docker Containers and
Images

A Docker container image describes a container environment. A Docker
container is an instance of that environment, running on Docker Engine.
You can run multiple containers from the same image, and all of them will
contain the same software and configuration, as specified in the image.

When you define a Docker image, you can use one or more layers, each of
which includes system libraries, dependencies and files needed for the
container environment. Image layers can be reused for different projects.

When a container runs, Docker adds a readable/writable top layer over the
static image layers. This top layer is used by the container to modify files
during runtime, and can also be used to customize the container. This way,
multiple containers created from the same image can have different data.

Parent and Base Images

There is a subtle technical different between parent and base images:

A base image is an empty container image, which allows advanced users to
create an image from scratch.

A parent image is a pre-configured image that provides some basic
functionality, such as a stripped-down Linux system, a database such as
MySQL or PostgreSQL, or a content management system such as WordPress.

However, in the container community, the terms “base image” and
“parent image” are often used interchangeably.

There is a large number of ready-made parent images available on
Docker Hub, and on many other public container repositories. You can
also use your own images as a parent for new images.

Containerized Architecture

A containerized architecture makes it possible to package software and its 
dependencies in an
isolated unit, called a container, which can run consistently in any environment. 
Containers are truly
portable, unlike traditional software deployment, in which software could not be 
moved to another
environment without errors and incompatibilities.

Containers are similar to virtual machines in a traditional virtualized architecture, 
but they are more
lightweight – they require less server resources and are much faster to start up. 
Technically, a
container differs from a virtual machine because it shares the operating system 
kernel with other
containers and applications, while a virtual machine runs a full virtual operating 
system.

Containerization helps developers and operations teams manage and automate software 
development
and deployment. Containerization makes it possible to define infrastructure as code 
(IaC) –
specifying required infrastructure in a simple configuration file and deploying it as 
many times as
needed. It is especially useful for managing microservices applications, which 
consist of a large
number of independent components.

Containers are a key part of the cloud native landscape, as defined by the Cloud 
Native Computing
Foundation (CNCF). They are an essential component of cloud native applications, 
built from the
ground up to leverage the elasticity and automation of the cloud.

Advantages of a Containerized
Architecture

Lower costs—on infrastructure operations, because you can run many containers on a 
single virtual machine.

Scalability—at the micro-service level eliminates the need to scale VMs or instances.

Instant replication—of microservices, enabled through deployment sets and replicas.

Flexible routing—you can set this up between services supported natively by 
containerization platforms.

Resilience—when a container fails, it’s easy to refresh/redeploy with a new container 
from the same image.

Full portability—between on-premise locations and cloud environments.

OS independent—there is no need to run an OS. All you need is to deploy a container 
engine on top of a host
OS.

Fast deployment—of new containers. You can also quickly terminate old containers 
using the same
environment.

Lightweight—since containers run without an OS, they are significantly lightweight 
and much less demanding
than images.

Faster “ready to compute”—you can start and stop containers within seconds—much 
faster than VMs.

Container security

Containers are units of software that allows you to deploy applications as
stand-alone, self-sufficient packages isolated from other activity on a
machine. Containers are similar to virtual machines (VMs), but do not run
an entire operating system. Instead, they share access to the operating
system (OS) kernel, which makes them faster and more lightweight than
VMs.

Container security requires a different approach compared to security in
traditional environments. Containers need a continuous security strategy
integrated into the entire software development lifecycle (SDLC). This
means securing the build pipeline, container images, container host
machines, container runtimes (such as Docker or containerd), container
platforms and orchestrators (such as Kubernetes), and application layers.

The Need for Container Security

Container adoption has grown exponentially in the past decade. Containers
are widely used as lightweight building blocks in software projects, and are
highly convenient because they contain everything needed to run an
application—code, runtime, tools, libraries, and configurations. A container
runs consistently every time regardless of the environment on the host
machine, is highly portable, and uses fewer resources compared to VMs.

Container engines like Docker and container orchestrators like Kubernetes
include some basic security controls, but they are not secure by default, and
hardening container runtimes and orchestrators can be complex.

Containerized application development also includes a large number of
third-party software components that can be vulnerable, making it critical to
scan container images at all stages of development. If there are
vulnerabilities in the container images, they are inherited by all containers
deployed from the image

Network Security

In a traditional architecture, network security relied on IP addresses.
However, in a containerized world, workloads are ephemeral and have
dynamically assigned IP addresses, making them more difficult to
secure.

Network segmentation and dynamic access controls are critical to
prevent lateral movement in case a container is compromised, while
ensuring containers can communicate with legitimate components.

Types of Container Security Solutions

Container Monitoring Solutions

A container monitoring solution allows security teams to track an application’s 
performance.
The ephemeral nature of containers makes them complex and thus harder to monitor than 
a
traditional application that runs on VMs or physical servers. Monitoring tools can 
collect and
analyze performance metrics at scale across large containerized environments, even as
workloads and clusters scale up and down.

Container Scanners

A container scanning or image scanning tool scans containers and related components 
to
identify security threats and detect vulnerabilities. Scanning is a crucial part of 
container
security, making this the most important tool for many security and DevOps teams 
dealing
with containerized workflows.

Container images are available from diverse sources, so maintaining trust in each 
container
image is essential. Container scanning allows teams to understand a container or 
container
image’s components and associated risks.

Trivy is a popular, open source container scanning tool that makes it easy to scan
vulnerabilities in containers, Kubernetes clusters, and infrastructure as code (IaC) 
templates.

Docker Image Security Best Practices

Prefer minimal base images—many Docker images use a fully installed operating system
distribution as their underlying image. If you don’t need general system libraries, 
avoid using base
images that install an entire operating system, or other components that are not 
essential for your
project, to limit the attack surface.

Least privileged user—Dockerfiles must always specify a USER, otherwise they will 
default to
running the container as root on the host machine. You should also avoid running 
applications on the
container with root privileges. Running as root can have severe security 
consequences, because
attackers compromising the container can gain control over the entire host.

Sign and verify images—you must make sure the image you are pulling to create your 
container is
really the image you selected from a trusted publisher, or created yourself. By using 
only signed
images, you can mitigate tampering with the image over the wire (a man in the middle 
attack), or
attackers pushing compromised images to a trusted repository.

Fix open source vulnerabilities—whenever you use a parent image in production, you 
need to be
able to trust all the components it deploys. Automatically scan images as part of 
your build process,
to ensure they do not contain vulnerabilities, security misconfigurations, or 
backdoors. Keep in mind
new vulnerabilities may be introduced over time, even in images that were originally 
verified as
secure.

Container Security Best Practices

Securing Images

Base images

Most images are created from “base images”. It is important to use a trusted image
provider. Anyone can publish images to Docker Hub.

Root access

Finally, just like containers should not run as root on the host, applications within 
a
container image should not be granted root access to their container. Create a 
dedicated
user account for each application and run it under this account. This is easy to 
define in a
Dockerfile.

Container Security Best Practices

Establish strict access control—define who can post new images,
delete and modify existing images. Periodically evaluate how many
people have access to the registry, remove unused accounts and revoke
unneeded permissions.

Scan images—when storing images in the registry, scan them for
vulnerabilities. It is not enough to scan images once—images that
were at one point safe can become a threat. New vulnerabilities are
discovered all the time, components can go out of date, and images
might be compromised by malicious insider or accidental update. To
address these issues, frequently scan all images stored in the registry.

Container Security Best Practices

Securing Deployment

When deploying containers, the most important thing is to ensure the target
environment is secure. This includes several aspects:

Hardening the underlying operating system containers run on.

Setting up virtual private cloud (VPC), security groups, and firewall rules.

Restricting access to container resources to a limited group of named
administrator accounts.

When using container orchestrators like Kubernetes, restricting API access
using role-based access control (RBAC) and hardening Kubernetes manifests.

Container Security Best Practices

Securing Runtime

Restricting container communications

Containers are often used to deploy components of microservices applications. Each 
application
component might connect to multiple other components and external services, 
increasing the
attack surface

Open ports

It is critical to expose only the port that the application serves and nothing else, 
both on
containers and container hosts. SSH is the only exception, but should also be closely 
controlled
and monitored

Storage volumes

Avoid bind-mounted storage volumes. Prefer to use volumes, and carefully manage them 
to
protect their data. Volumes should preferably be read only, and creation and updates 
of storage
volumes should be carefully controlled

Ensuring only safe images run

Even if all images are scanned to and ensure they are secure, there is always the 
possibility that
another, unsafe image will be the one actually running in a production environment


II
Monolithic vs. Microservices
Architecture

Microservices are an architectural and organizational approach to
software development where software is composed of small
independent services that communicate over well-defined APIs.

Benefits of Microservices

Agility

Microservices foster an organization of small, independent teams that take ownership 
of their services

Flexible Scaling

Microservices allow each service to be independently scaled to meet demand for the 
application feature it
supports

Easy Deployment

Microservices enable continuous integration and continuous delivery, making it easy 
to try out new ideas and to
roll back if something doesn’t work

Technological Freedom

Microservices architectures don’t follow a “one size fits all” approach. Teams have 
the freedom to choose the
best tool to solve their specific problems

Reusable Code

Dividing software into small, well-defined modules enables teams to use functions for 
multiple purposes

Resilience

Service independence increases an application’s resistance to failure. In a 
monolithic architecture, if a single
component fails, it can cause the entire application to fail

Conteinteiners vs Virtual Machines

Virtual machines(VMs) are a
technology for building virtualized
computing environments. They have
been around for quite a while and are
considered the foundation of the first
generation of cloud computing

Containers are a lighter-weight, more
agile way of handling virtualization —
since they don't use a hypervisor, you
can enjoy faster
resource provisioning and speedier
availability of new applications.

Orchestration

Kubernetes vs Docker
Compose

Kubernetes and Docker
Compose are both container
orchestration frameworks.
Kubernetes runs containers
over a number of computers,
virtual or real. Docker Compose
runs containers on a single host
machine

SAST

SAST takes place very early in the software development life cycle
(SDLC) as it does not require a working application and can take
place without code being executed.

SAST tools give developers real-time feedback as they code, helping
them fix issues before they pass the code to the next phase of the
SDLC.

Developers can also create the customized reports they need with
SAST tools; these reports can be exported offline and tracked using
dashboards

It’s important to note that SAST tools must be run on the application
on a regular basis, such as during daily/monthly builds, every time
code is checked in, or during a code release

Why is SAST an important

Developers dramatically outnumber security staff. It can be
challenging for an organization to find the resources to perform
code reviews on even a fraction of its applications

A key strength of SAST tools is the ability to analyze 100% of
the codebase

SAST tools automatically identify critical vulnerabilities—such
as buffer overflows, SQL injection, cross-site scripting, and
others—with high confidence

key steps to run SAST effectively

Finalize the tool

Select a static analysis tool that can perform code reviews of applications written 
in the programming languages
you use

Create the scanning infrastructure, and deploy the tool

This step involves handling the licensing requirements, setting up access control and 
authorization, and procuring
the resources required (e.g., servers and databases) to deploy the tool

Customize the tool

Fine-tune the tool to suit the needs of the organization. ntegrate the tool into the 
build environment, create
dashboards for tracking scan results, and build custom reports

Prioritize and onboard applications

Once the tool is ready, onboard your applications. If you have a large number of 
applications, prioritize the
high-risk applications to scan first.

Analyze scan results

his step involves triaging the results of the scan to remove false positives. Once 
the set of issues is finalized, they
should be tracked and provided to the deployment teams for proper and timely 
remediation

Provide governance and training

Proper governance ensures that your development teams are employing the scanning 
tools properly

Dependency-Check

Dependency-Check is a Software Composition Analysis (SCA) tool
that attempts to detect publicly disclosed vulnerabilities contained
within a project’s dependencies. It does this by determining if there is
a Common Platform Enumeration (CPE) identifier for a given
dependency

What is Container Scanning?

Container scanning, or container image scanning, is the process
and scanning tools used to identify vulnerabilities within
containers and their components. It’s key to container security,
and enables developers and cybersecurity teams to fix security
threats in containerized applications before deployment.

Container scanning basics

Vulnerabilities can be introduced to containers in a number of ways: from the
software inside the container, how the container interacts with the host operating
system and adjacent containers, the configurations for networking and storage,
and more. A container scanner is an automated tool that analyzes these various
container components to detect security vulnerabilities.

Besides vulnerabilities introduced directly by the code and tool you add to an
image, issues can originate from other images that your containers rely on. These
other images are called parent images or base images (though “parent” is
technically more correct). In fact, your container image may be based on a publicly
available image that contains known vulnerabilities and malware, especially if you
didn’t download the image from a verified publisher and authenticate the image
publisher and contents. Even images from well known and trusted providers often
have vulnerabilities, but by scanning for vulnerabilities in your image, and
identifying your parent images and their vulnerabilities, you can often remediate a
large number of issues with a single change.

Security scanners can be integrated during various stages of development. For 
example, you can scan potential
parent images from your desktop before deciding which one to use as the base for your 
image. Some tools and IDE
plugins will scan Dockerfiles and indicate alternative images you could select that 
have fewer vulnerabilities or are
slimmer in size. Many organizations integrate container vulnerability scanning into 
the continuous integration and
continuous delivery (CI/CD) pipelines, which is where the “real” images are often 
built prior to deployment. Scanning
in your pipelines allows you to prevent container images with too many issues from 
being stored in your registries
and from reaching production. Most teams also monitor containerized deployments when 
they’re running on
Kubernetes or another platform. Container security scanning, therefore, can 
dramatically improve the security of an
application without a lot of additional effort by developers.

What kinds of Container Vulnerabilities can
be detected?

There are a variety of ways that vulnerabilities can be introduced into a container. 
These
container vulnerabilities can range from insecure application code and runtime
misconfigurations to network threats and access control issues. Protecting against 
these
threats requires continuous container monitoring and up-to-date knowledge of new
vulnerabilities as they’re discovered.

Most container scanning solutions leverage a public source for vulnerability 
information like
the National Vulnerability Database (NVD) or the Common Vulnerabilities & Exposures
(CVE) database. These databases publish known exploits to enable
automated vulnerability management, security measurement, and compliance.

How container layers impact vulnerability
detection

Containers are building blocks for modern applications, and one advantage of using 
containers is that you can build
upon the work of others. Docker made this easy for developers and made it easy to 
share containers images via
Docker Hub. Docker also supplies its own Docker Official Images that cover a wide 
range of language and
frameworks and other application services. As you build up your own images on top of 
the work of others, your
containers end up with multiple read-only layers, and one final read/write layer.

Each new layer has the risk of introducing new vulnerabilities into the container, so 
it’s crucial that the container
scanner you use can detect issues layer by layer. It’s even better if they relate 
those layers back to the commands or
Dockerfile instruction that created them, since that’s a more familiar place for 
developers to make a change.
Furthermore, since images are almost always built on top of other images (via the 
FROM command in a Dockerfile,
for example) a container vulnerability scanner like Snyk Container that can determine 
that certain layers are actually
a Docker Official Image and intelligently provide alternatives that reduce 
vulnerabilities, is even better than just
providing the raw layers.


I
DevOps is a combination of
software development (dev) and
operations (ops). It is defined as
a software engineering
methodology which aims to
integrate the work of
development teams and
operations teams by facilitating a
culture of collaboration and
shared responsibility.

Core DevOps principles

Automation of the software development lifecycle. This includes automating testing, 
builds, releases, the provisioning of
development environments, and other manual tasks that can slow down or introduce 
human error into the software delivery process.

Collaboration and communication. A good DevOps team has automation, but a great 
DevOps team also has effective collaboration
and communication.

Continuous improvement and minimization of waste. From automating repetitive tasks to 
watching performance metrics for ways
to reduce release times or mean-time-to-recovery, high performing DevOps teams are 
regularly looking for areas that could be
improved.

Hyperfocus on user needs with short feedback loops. Through automation, improved 
communication and collaboration, and
continuous improvement, DevOps teams can take a moment and focus on what real users 
really want, and how to give it to them.

The four phases of DevOps

Phase 1: Bring Your Own DevOps In the Bring Your Own DevOps
phase, each team selected its own tools. This approach caused
problems when teams attempted to work together because they were
not familiar with the tools of other teams.

Phase 2: Best-in-class DevOps To address the challenges of using
disparate tools, organizations moved to the second phase, Best-in-class
DevOps. In this phase, organizations standardized on the same set of
tools, with one preferred tool for each stage of the DevOps lifecycle. It
helped teams collaborate with one another, but the problem then
became moving software changes through the tools for each stage.

The four phases of DevOps

Phase 3: Do-it-yourself DevOps To remedy this problem, organizations adopted 
do-it-yourself (DIY) DevOps, building
on top of and between their tools. They performed a lot of custom work to integrate 
their DevOps point solutions together.
However, since these tools were developed independently without integration in mind, 
they never fit quite right. For many
organizations, maintaining DIY DevOps was a significant effort and resulted in higher 
costs, with engineers maintaining
tooling integration rather than working on their core software product.

Phase 4: DevOps Platform A single-application platform approach improves the team 
experience and business efficiency.
A DevOps platform replaces DIY DevOps, allowing visibility throughout and control 
over all stages of the DevOps
lifecycle. By empowering all teams – Development, Operations, IT, Security, and 
Business – to collaboratively plan, build,
secure, and deploy software across an end-to-end unified system, a DevOps platform 
represents a fundamental step-change
in realizing the full potential of DevOps.

The DevOps lifecycle and how DevOps
works

Plan

Organize the work that needs to be done, prioritize it, and track its completion.

Create

Write, design, develop and securely manage code and project data with your team.

Verify

Ensure that your code works correctly and adheres to your quality standards —
ideally with automated testing.

Package

Package your applications and dependencies, manage containers, and build artifacts.

The DevOps lifecycle and how DevOps
works

Secure

Check for vulnerabilities through static and dynamic tests, fuzz testing, and
dependency scanning.

Release

Deploy the software to end users.

Configure

Manage and configure the infrastructure required to support your applications.

Monitor

Track performance metrics and errors to help reduce the severity and frequency of
incidents.

Govern

Manage security vulnerabilities, policies, and compliance across your organization.

DevOps tools, concepts and fundamentals

Version control

The fundamental practice of tracking and
managing every change made to source code
and other files. Version control is closely
related to source code management.

Agile

Agile development means taking iterative,
incremental, and lean approaches to
streamline and accelerate the delivery of
projects.

Continuous Integration (CI)

The practice of regularly integrating all code
changes into the main branch, automatically
testing each change, and automatically kicking
off a build.

Continuous Delivery (CD)

Continuous delivery works in conjunction with
continuous integration to automate the
infrastructure provisioning and application
release process. They are commonly referred
to together as [CI/CD](/topics/ci-cd/).

Shift left

A term for shifting security and testing much
earlier in the development process. Doing this
can help speed up development while
simultaneously improving code quality.

How does DevSecOps relate to DevOps?

Security has become an integral part of the software development lifecycle,
with much of the security shifting left in the development
process. DevSecOps ensures that DevOps teams understand the security and
compliance requirements from the very beginning of application creation
and can properly protect the integrity of the software.

By integrating security seamlessly into DevOps workflows, organizations
gain the visibility and control necessary to meet complex security demands,
including vulnerability reporting and auditing. Security teams can ensure
that policies are being enforced throughout development and deployment,
including critical testing phases.

DevSecOps can be implemented across an array of environments such as
on-premises, cloud-native, and hybrid, ensuring maximum control over the
entire software development lifecycle.

What is DevSecOps?

DevSecOps (short for development, security, and operations) is
a development practice that integrates security initiatives at
every stage of the software development lifecycle to deliver
robust and secure applications.

Security has traditionally come at the end of the development
lifecycle, adding cost and time when code is inevitably sent back to
the developer for fixes. DevSecOps — a combination of development,
security, and operations — is an approach to software development
that integrates security throughout the development lifecycle.

DevSecOps vs. DevOps

DevOps practices enable software developers (devs) and operations (ops)
teams to accelerate delivery through automation, collaboration, fast
feedback, and iterative improvement.

Although the term DevSecOps looks like DevOps with the Sec inserted in
the middle, it’s more than the sum of its parts. DevSecOps is an evolution of
DevOps that weaves application security practices into every stage of
software development right through deployment with the use of tools and
methods to protect and monitor live applications. New attack surfaces such
as containers and orchestrators must be monitored and protected alongside
the application itself. DevSecOps tools automate security workflows to
create an adaptable process for your development and security teams,
improving collaboration and breaking down silos.

DevSecOps fundamentals

Automation

GitLab’s 2022 DevSecOps Survey found that a majority of DevOps teams are
running static application security testing (SAST), dynamic application
security testing (DAST), or other security scans regularly, but fewer than a
third of developers actually get those results in their workflow. A majority of
security pros say their DevOps teams are shifting left, and 47% of teams report
full test automation.

Collaboration

A single source of truth that reports vulnerabilities and remediation provides
much-needed transparency to both development and security team. It can
streamline cycles, eliminate friction, and remove unnecessary translation
across tools.

DevSecOps fundamentals

Policy guardrails

Every enterprise has a different appetite for risk. Your security policies will
reflect what is right for you while the regulatory requirements to which you
must adhere will also influence the policies you must apply. Hand-in-hand
with automation, guardrails can ensure consistent application of your security
and compliance policies.

Visibility

An end-to-end DevSecOps platform can give auditors a clear view into who
changed what, where, when, and why from beginning to end of the software
lifecyle. Leveraging a single source of truth can also ensure earlier visibility
into application risks.

Benefits of DevSecOps

Enhanced Application Security
DevSecOps embeds a proactive approach to mitigate cybersecurity
threats early in the development lifecycle

Cross-team ownership
DevSecOps brings development teams and application security
teams together early in the development process, building a
collaborative cross-team approach.

Limit Security Vulnerabilities
Leverage automation to identify, manage, and patch common
vulnerabilities and exposures (CVE). Use pre-built scanning solutions
early and often to scan any prebuilt container images in the build
pipeline for CVEs.

Benefits of DevSecOps

Proactively find and fix vulnerabilities

Unlike traditional approaches where security is often left to the end, DevSecOps 
shifts security to earlier in the
software development lifecycle. By reviewing, scanning, and testing code for security 
issues throughout the
development process, teams can identify security concerns proactively and address 
them immediately, before
additional dependencies are introduced or code is released to customers.

Release more secure software, faster

If security vulnerabilities aren’t detected until the end of a project, the result 
can be major delays as development
teams scramble to address the issues at the last minute. But with a DevSecOps 
approach, developers can
remediate vulnerabilities while they’re coding, which teaches secure code writing and 
reduces back and forth
during security reviews. Not only does this help organizations release software 
faster, it ensures that their
software is more secure and cost efficient.

Keep pace with modern development methods

Customers and business stakeholders demand software that is fast, reliable, and 
secure. To keep up, development
teams need to leverage the latest in collaborative and security technology, including 
automated security testing,
continuous integration and continuous delivery (CI/CD), and vulnerability patching. 
DevSecOps is all about
improving collaboration between development, security, and operations teams to 
improve organizational
efficiency and free up teams to focus on work that drives value for the business.

Why is DevSecOps Important

DevSecOps is important in today’s business environment to
mitigate the rising frequency of cyber-attacks. By implementing
security initiatives early and often, applications in an array of
industries achieve the following benefits.

How does DevSecOps Work

Code
The first step to a development approach that aligns with DevSecOps is to code in
segments that are both secured and trusted

Build
To take code and deliver comprehensive container images that contain a core OS

Prep
Before deployment, organizations need to ensure their application complies with
security policies

Deploy
Scans delivered in previous steps give organizations a comprehensive
understanding of the application’s security strength

Run
As deployments run, SecOps teams can leverage active deployment analytics,
monitoring and automation to ensure continuous compliance while also mitigating
the risk of vulnerabilities that surface following deployment.

Creating a DevSecOps culture

Remember that security and security professionals are valuable assets, not
bottlenecks or barriers. Don’t lose sight of the wider perspective: a vulnerability 
that isn’t
detected until later in the process is going to be much harder and more expensive to 
fix.

Work in small iterations. By delivering code in small chunks, you’ll be able to 
detect
vulnerabilities more quickly.

Allow everyone to contribute. Establish a norm where everyone can comment and
suggest improvement to code and processes. Encouraging everyone on the team to submit
changes jumpstarts collaboration and makes it everyone’s responsibility to improve 
the
process.

Always be ready for an audit. Ensure that everyone on the team understands the
importance of compliance and establish norms for collecting and updating compliance
information.

Train everyone on security best practices. Ensure that your development and 
operations
teams are well trained in secure development by providing detailed security 
guidelines and
hands-on training.

Key Elements for Implementing DevSecOps

Every DevSecOps project is unique, but there are common elements
most organizations will need to implement DevOps successfully.

Container Security

Infrastructure Automation

Application Analysis

Identity and Access Management

Network Controls and Segmentation

Data Controls

Auditing, Monitoring, and Alerting

Remediation
